{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8 - Recognizing objects in images with deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get to know the main ingredients of deep learning and get started using the GPUs available in the Big Data Lab.\n",
    "\n",
    "You'll learn to use\n",
    "\n",
    " * tensors\n",
    " * automatic differentiation\n",
    " * layered learners\n",
    " * p(re)trained networks for image classification.\n",
    "\n",
    "## Check the GPU setup\n",
    "\n",
    "When you are logged in to a lab machine, run ``nvidia-smi`` to see the available card and its memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ nvidia-smi\n",
    "Mon Feb  5 08:03:15 2018       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 390.12                 Driver Version: 390.12                    |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  GeForce GTX 105...  Off  | 00000000:01:00.0  On |                  N/A |\n",
    "| 45%   24C    P8    N/A /  75W |   3087MiB /  4038MiB |      0%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                       GPU Memory |\n",
    "|  GPU       PID   Type   Process name                             Usage      |\n",
    "|=============================================================================|\n",
    "|    0      3627      G   /usr/lib/xorg/Xorg                           169MiB |\n",
    "|    0     10843      C   ...d/CMPT/big-data/tmp_py/dlenv/bin/python  2897MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the machine has an **NVIDIA GTX 1050 with 4G of RAM**. Also, you can see that I'm running a process (pid=10843) that currently takes up close to 3 GB of GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ pstree -ls 10843\n",
    "screen───bash───jupyter-noteboo───python─┬─4*[python]\n",
    "                                         └─26*[{python}]\n",
    "```\n",
    "Inside a terminal window you may use ``who``, ``ps -aux | less``, or ``pstree -ls <PID>`` as above to find out who is using the shared resources. In my case, it turns out that I'm running a jupyter notebook related to process 10843. Halting the notebook frees up the GPU memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch setup in the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we are going to use **[pytorch](http://pytorch.org)**, which received some praise recently for being faster than [tensorflow](http://tensorflow.org) and for also having a nice high-level API as NN modules that are similar to [Keras](https://keras.io/).\n",
    "\n",
    "The default `conda` environment has pytorch 1.3.1 installed. This means, you should be able to use it without any changes to your environment.\n",
    "If you activate the `gt` environment, as shown at the end of [Assignment 6](https://github.com/sfu-db/bigdata-cmpt733/blob/master/Assignments/A6/A6.ipynb) you can use pytorch version 1.4 and tensorflow 2.1 (not required in this assignment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use our downloaded pre-built models\n",
    "To save disk space in your home folder, we recommend that you let pytorch use the pre-built models that we already downloaded for you (about 1.9G):\n",
    "```\n",
    "mkdir -p ~/.cache/torch/checkpoints\n",
    "ln -s /usr/shared/CMPT/big-data/dot_torch_shared/checkpoints/* ~/.cache/torch/checkpoints\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an aside, since `~/.cache` can contain several folders (e.g. `pip` package downloads) whose size can exceed your quota, it may be a good idea to create a `dotcache` folder in your scratch space and create a symbolic link to it from `~/.cache`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn about Pytorch usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To familiarize yourself with PyTorch, have a look at the [Examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html) or briefly skim over the [60 min blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Finding rectangles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice blog-post by [Johannes Rieke](https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491) presents a simple setup from scratch that finds rectangles in a black & white image. In order to play with it, we just have to translate a few calls from Keras to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To familiarize yourself with using pytorch, have a look at the [Examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to check GPU memory, uncomment and run the following line\n",
    "!{'nvidia-smi'}\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "# K.tensorflow_backend._get_available_gpus()\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-73ad29cc0a72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Create images with random rectangles and bounding boxes. \n",
    "num_imgs = 50000\n",
    "\n",
    "img_size = 8\n",
    "min_object_size = 1\n",
    "max_object_size = 4\n",
    "num_objects = 1\n",
    "\n",
    "bboxes = np.zeros((num_imgs, num_objects, 4))\n",
    "imgs = np.zeros((num_imgs, img_size, img_size))  # set background to 0\n",
    "\n",
    "for i_img in range(num_imgs):\n",
    "    for i_object in range(num_objects):\n",
    "        w, h = np.random.randint(min_object_size, max_object_size, size=2)\n",
    "        x = np.random.randint(0, img_size - w)\n",
    "        y = np.random.randint(0, img_size - h)\n",
    "        imgs[i_img, x:x+w, y:y+h] = 1.  # set rectangle to 1\n",
    "        bboxes[i_img, i_object] = [x, y, w, h]\n",
    "        \n",
    "imgs.shape, bboxes.shape\n",
    "\n",
    "display(Markdown('**Here is an example of the training data:**'))\n",
    "i = 0\n",
    "plt.imshow(imgs[i].T, cmap='Greys', interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
    "for bbox in bboxes[i]:\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec='r', fc='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape and normalize the image data to mean 0 and std 1. \n",
    "X = (imgs.reshape(num_imgs, -1) - np.mean(imgs)) / np.std(imgs)\n",
    "X.shape, np.mean(X), np.std(X)\n",
    "\n",
    "# Normalize x, y, w, h by img_size, so that all values are between 0 and 1.\n",
    "# Important: Do not shift to negative values (e.g. by setting to mean 0), because the IOU calculation needs positive w and h.\n",
    "y = bboxes.reshape(num_imgs, -1) / img_size\n",
    "y.shape, np.mean(y), np.std(y)\n",
    "\n",
    "# Split training and test.\n",
    "i = int(0.8 * num_imgs)\n",
    "train_X = X[:i]\n",
    "test_X = X[i:]\n",
    "train_y = y[:i]\n",
    "test_y = y[i:]\n",
    "test_imgs = imgs[i:]\n",
    "test_bboxes = bboxes[i:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a\n",
    "Construct a Pytorch model that resembles the Keras one in the original blog post, i.e. have a fully connected, hidden layer with 200 neurons, ReLU nonlinearity and dropout rate of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Model for Task 1a\n",
    "NUM_CLASSES = 4\n",
    "model_task1a = torch.nn.Sequential(\n",
    "    torch.nn.Linear(img_size*img_size, 200),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(200, NUM_CLASSES)\n",
    ")\n",
    "\n",
    "# Model for training, Set hidden nodes = 400\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(img_size*img_size, 400),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.2),\n",
    "    torch.nn.Linear(400, NUM_CLASSES)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = torch.optim.Adadelta(model.parameters())\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = Variable(torch.Tensor(train_X))\n",
    "# labels = Variable(torch.Tensor(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phase = 'train'\n",
    "# running_loss = 0.0\n",
    "# running_corrects = 0\n",
    "\n",
    "# loss_record = []\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     outputs = model(inputs)\n",
    "\n",
    "#     loss = loss_fn(outputs, labels)\n",
    "\n",
    "#     if phase == 'train':\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     running_loss += loss.data * inputs.size(0)\n",
    "#     epoch_loss = running_loss / inputs.shape[0] / (epoch+1)\n",
    "#     loss_record.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "phase = 'test'\n",
    "# change the model from training to evaluation mode to improve testing performance\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split train_set to train and validation set with .8/.2 ratio\n",
    "copytrain_X = train_X.copy()\n",
    "copytrain_y = train_y.copy()\n",
    "train_X, val_X, train_y, val_y = train_test_split(copytrain_X, copytrain_y, test_size=1, random_state=42)\n",
    "\n",
    "train_inputs = Variable(torch.Tensor(train_X)).to(device)\n",
    "train_labels = Variable(torch.Tensor(train_y)).to(device)\n",
    "val_inputs = Variable(torch.Tensor(val_X)).to(device)\n",
    "val_labels = Variable(torch.Tensor(val_y)).to(device)\n",
    "# val_inputs = Variable(torch.Tensor(test_X)).to(device)\n",
    "# val_labels = Variable(torch.Tensor(test_y)).to(device)\n",
    "train_running_loss = 0.0\n",
    "val_running_loss = 0.0\n",
    "\n",
    "train_loss_record = []\n",
    "val_loss_record = []\n",
    "\n",
    "for epoch in range(800):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    train_outputs = model(train_inputs)\n",
    "\n",
    "    train_loss = loss_fn(train_outputs, train_labels)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    train_running_loss += train_loss.data * train_inputs.size(0)\n",
    "    train_epoch_loss = train_running_loss / train_inputs.shape[0] / (epoch+1)\n",
    "    train_loss_record.append(train_epoch_loss)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        yhat = model(val_inputs)\n",
    "        val_loss = loss_fn( yhat,val_labels)\n",
    "\n",
    "        val_running_loss += val_loss.data * val_inputs.size(0)\n",
    "        val_epoch_loss = val_running_loss / val_inputs.shape[0] / (epoch+1)\n",
    "        val_loss_record.append(val_epoch_loss)\n",
    "plt.plot(train_loss_record,'r')\n",
    "plt.plot(val_loss_record,'b')\n",
    "# val_loss might be lower than train_loss because of Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict bounding boxes on the test images.\n",
    "test_inputs = Variable(torch.Tensor(test_X)).to(device)\n",
    "pred_y = model(Variable(test_inputs))\n",
    "pred_bboxes = pred_y.data * img_size\n",
    "pred_bboxes = pred_bboxes.cpu()\n",
    "pred_bboxes = pred_bboxes.numpy().reshape(len(pred_bboxes), num_objects, -1)\n",
    "pred_bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(bbox1, bbox2):\n",
    "    '''Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity'''\n",
    "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n",
    "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
    "\n",
    "    w_I = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
    "    h_I = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
    "    if w_I <= 0 or h_I <= 0:  # no overlap\n",
    "        return 0.\n",
    "    I = w_I * h_I\n",
    "    U = w1 * h1 + w2 * h2 - I\n",
    "    return I / U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few images and predicted bounding boxes from the test dataset. \n",
    "plt.figure(figsize=(12, 3))\n",
    "for i_subplot in range(1, 5):\n",
    "    plt.subplot(1, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_imgs))\n",
    "    plt.imshow(test_imgs[i].T, cmap='Greys', interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox in zip(pred_bboxes[i], test_bboxes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], ec='r', fc='none'))\n",
    "        plt.annotate('IOU: {:.2f}'.format(IOU(pred_bbox, exp_bbox)), (pred_bbox[0], pred_bbox[1]+pred_bbox[3]+0.2), color='r')\n",
    "# Calculate the mean IOU (overlap) between the predicted and expected bounding boxes on the test dataset. \n",
    "summed_IOU = 0.\n",
    "for pred_bbox, test_bbox in zip(pred_bboxes.reshape(-1, 4), test_bboxes.reshape(-1, 4)):\n",
    "    summed_IOU += IOU(pred_bbox, test_bbox)\n",
    "mean_IOU = summed_IOU / len(pred_bboxes)\n",
    "mean_IOU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b:\n",
    "Move the computation that is currently done on the CPU over to the GPU using CUDA and increase the number of epochs. Improve the training setup until you **reach a test IOU above 0.9**.\n",
    "\n",
    "You can make the changes that move computation to the GPU directly in the cells above.\n",
    "\n",
    "You may get stuck not achieving test IOU above 0.6. In that case, learn about switching the model to evaluation mode and apply the change above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1c:\n",
    "Why does `eval` mode above have such a significant effect on test performance? Please give a short answer below.\n",
    "\n",
    "ANSWER:\n",
    "Because some layers have different behavior during train and evaluation, we \n",
    "need to explicitly tell which part is train and evaluation part. \n",
    "For example, Dropout layer is used for training data (to prevent overfitting problem) but we don't use it during evaluation phase.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Use a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in class, deep learning systems are hardly ever developed from scratch, but usually work by refining existing solutions to similar problems. For the following task, we'll **work through the \n",
    "[Transfer learning tutorial](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)**, which also provides a ready-made jupyter notebook.\n",
    "\n",
    " 1. Download the notebook and get it to run in your environment. This also involves downloading the bees and ants dataset.\n",
    " 2. Perform your own training with the provided setup, fill out the answer to Task 2.2 below.\n",
    " 3. Change the currently chosen pretrained network (resnet) to a different one. At least try out VGG and one other type and use the \"conv net as fixed feature extractor\" approach, fill out the answer to Task 2.3 below.\n",
    " 4. Load a picture that you took yourself and classify it with an unmodified pretrained network (e.g. the original VGG network without the modificiation that you applied in 2.3 above) so that you can detect one out of 1000 classes. Please fill out your the answer to Task 2.4 below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your solution for Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start, get the data from [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip) and extract it into a subfolder `data`. The following import is going to attempt loading the image data from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize much of the source code from the tutorial notebook located at\n",
    "https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "using [this module](https://github.com/sfu-db/bigdata-cmpt733/blob/master/Assignments/A8/tfl_tut.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfl_tut import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please study the original notebook and then continue to use its functions as imported from the `tfl_tut` model for convenience to minimize source code copy & paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Task 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO paste and maybe modify relevant code to perform your own training\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.SGD(model_conv.parameters(), lr=0.001, momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "model_conv.to(device)\n",
    "model_ft = train_model(model_conv, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Task 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hints for this task\n",
    "\n",
    "Focus on the section **Conv net as fixed feature xtractor** of the transfer learning tutorial.\n",
    "First, change the line\n",
    "```\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "```\n",
    "to load VGG16 instead. Set all its parameters to *not* require gradient computation, as shown in the tutorial.\n",
    "\n",
    "Next, print out the new `model_conv` and identify the last step of the classification. This is not named the same way as the ```fc``` layer for resnet, but it works similarily. The last classification step of the VGG model determines the probabilities for each of the 1000 classes of the dataset. Change this layer to identify only 2 classes to distinguish ants and bees as in the example.\n",
    "\n",
    "To change the structure of some `Sequential` component called ```model_conv.module_name``` and to modify its last layer into a `DifferentLayer` type, you can use this syntax:\n",
    "\n",
    "```\n",
    "nn.Sequential(*list(model_conv.module_name.children())[:-1] +\n",
    "                     [nn.DifferentLayer(...)])\n",
    "```\n",
    "and replace the old `model_conv.module_name` with this differently structured version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "model_conv = torchvision.models.vgg16(pretrained=False)\n",
    "\n",
    "# Last layer of original VGG16 has 4096 in_node and 1000 out_node (classes)\n",
    "model_conv_temp = nn.Sequential(*list(model_conv.children())[:-1])\n",
    "new_model_conv = torch.nn.Sequential(model_conv_temp, torch.nn.Linear(4096,2))\n",
    "# print(nn.Sequential(*list(new_model_conv.children())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Task 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Get class labels \n",
    "LABELS_URL = 'https://s3.amazonaws.com/outcome-blog/imagenet/labels.json'\n",
    "response = requests.get(LABELS_URL)  # Make an HTTP GET request and store the response.\n",
    "labels = {int(key): value for key, value in response.json().items()}\n",
    "\n",
    "model_vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "model_vgg16.eval()\n",
    "\n",
    "img1 = Image.open(\"dog1.jpg\")\n",
    "img_transforms = transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "img1_tensor = img_transforms(img1)\n",
    "\n",
    "prediction1 = model_vgg16(img1_tensor.unsqueeze(0))\n",
    "prediction1 = prediction1.data.numpy().argmax()\n",
    "\n",
    "print(labels[prediction1])\n",
    "plt.imshow(img1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = Image.open(\"burger.jpg\")\n",
    "img2_tensor = img_transforms(img2)\n",
    "prediction2 = model_vgg16(img2_tensor.unsqueeze(0))\n",
    "prediction2 = prediction2.data.numpy().argmax()\n",
    "print(labels[prediction2])\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please include the picture and its class label in the saved notebook. It's OK, if we don't have the actual image file to reproduce the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your submission should be based on a modified version of [this notebook](https://github.com/sfu-db/bigdata-cmpt733/blob/master/Assignments/A8/A8.ipynb) containing answers to Task 1 and for Task 2 including some portions of the transfer learning tutorial notebook in the sections above corresponding to tasks 2.1 - 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
