{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Entity Resolution (Part 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Assignment 2 (Part 2), you will learn how to use Active Learning to address the entity resolution problem. After completing this assignment, you should be able to answer the following questions:\n",
    "\n",
    "1. Why Active Learning?\n",
    "2. How to implement uncertain sampling, a popular query strategy for Active Learning?\n",
    "3. How to solve an ER problem using Active Learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Active learning](http://tiny.cc/al-wiki) is a certain type of ML algorithms that can train a high-quality ML model with small data-labeling cost. Its basic idea is quite easy to understand. Consider a typical supervised ML problem, which requires a (relatively) large training dataset. In the training dataset, there may be only a small number of data points that are beneficial to the trained ML model. In other words, labeling a small number of data points is enough to train a high-quality ML model. The goal of active learning is to help us to identify those data points. \n",
    "\n",
    "\n",
    "In this assignment, we will develop an Active Learning approach for Entity Resolution. The following figure shows the architecture of an entity resolution solution. It consists of four major steps. **I will provide you the source code for Steps 1, 2, 4. Your job is to implement Step 3.**  \n",
    "\n",
    "<img src=\"img/arch.png\", width=800/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we get a restaurant dataset `restaurant.csv`. The data has many duplicate restaurants.  For example, the first two rows shown below are duplicated (i.e., refer to the same real-world entity). You can check out all duplicate (matching) record pairs from `true_matches.json`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('restaurant.csv')\n",
    "data = df.values.tolist()\n",
    "print(\"(#Rows, #Cols) :\", df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Similar Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use a similarity-join algorithm to generate similar pairs. \n",
    "\n",
    "Below is the code. After running the code, we get 678 similar pairs ordered by their similarity decreasingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Pairs:  367653.0\n",
      "Num of Similar Pairs:  678\n",
      "The Most Similar Pair:  ([170, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern/soul'], [169, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern'])\n"
     ]
    }
   ],
   "source": [
    "from a2_utils import *\n",
    "\n",
    "data = df.values.tolist()\n",
    "simpairs = simjoin(data)\n",
    "\n",
    "print(\"Num of Pairs: \", len(data)*(len(data)-1)/2)\n",
    "print(\"Num of Similar Pairs: \", len(simpairs))\n",
    "print(\"The Most Similar Pair: \", simpairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `simjoin` helps us remove the number of pairs from 367653 to 678. But, there are still many non-matching pairs in `simpairs` (see below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([764, \"buzio's in the rio\", '3700 w. flamingo rd.', 'las vegas', 'seafood'], [542, 'carnival world', '3700 w. flamingo rd.', 'las vegas', 'buffets'])\n"
     ]
    }
   ],
   "source": [
    "print(simpairs[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use active learning to train a classifier, and then use the classifier to classify each pair in `simpairs` as either \"matching\" or \"nonmatching\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of similar pairs, what you need to do next is to iteratively train a classifier to decide which pairs are truly matching. We are going to use [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) as our classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning, all the pairs are unlabeled. To initialize a model, we first pick up ten pairs and then label each pair using  the `crowdsourcing()` function. You can assume that `crowdsourcing()` will ask a crowd worker (e.g., on Amazon Mechanical Turk) to label a pair. \n",
    "\n",
    "\n",
    "`crowdsourcing(pair)` is a function that simulates the use of crowdsourcing to label a pair\n",
    "  \n",
    "  - **Input:**\tpair – A pair of records \n",
    "\n",
    "  - **Output:**\tBoolean –  *True*: The pair of records are matching; *False*: The pair of records are NOT matching;\n",
    "\n",
    "Please use the following code to do the initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([170, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern/soul'], [169, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern']), ([88, 'manhattan ocean club', '57 w. 58th st.', 'new york city', 'seafood'], [87, 'manhattan ocean club', '57 w. 58th st.', 'new york', 'seafood'])]\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[170, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern/soul']\n",
      "[169, \"mary mac's tea room\", '224 ponce de leon ave.', 'atlanta', 'southern']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[88, 'manhattan ocean club', '57 w. 58th st.', 'new york city', 'seafood']\n",
      "[87, 'manhattan ocean club', '57 w. 58th st.', 'new york', 'seafood']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[112, 'san domenico', '240 central park s.', 'new york city', 'italian']\n",
      "[111, 'san domenico', '240 central park s', 'new york', 'italian']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[197, 'fleur de lys', '777 sutter st.', 'san francisco', 'french (new)']\n",
      "[196, 'fleur de lys', '777 sutter st.', 'san francisco', 'french']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[8, 'cafe bizou', '14016 ventura blvd.', 'sherman oaks', 'french bistro']\n",
      "[7, 'cafe bizou', '14016 ventura blvd.', 'sherman oaks', 'french']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[709, 'arcadia', '21 e. 62nd st.', 'new york city', 'american (new)']\n",
      "[66, 'four seasons', '99 e. 52nd st.', 'new york city', 'american (new)']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[709, 'arcadia', '21 e. 62nd st.', 'new york city', 'american (new)']\n",
      "[70, 'gramercy tavern', '42 e. 20th st.', 'new york city', 'american (new)']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[729, 'la grenouille', '3 e. 52nd st.', 'new york city', 'french (classic)']\n",
      "[60, 'daniel', '20 e. 76th st.', 'new york city', 'french (new)']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[733, 'menchanko-tei', '39 w. 55th st.', 'new york city', 'japanese']\n",
      "[76, 'la caravelle', '33 w. 55th st.', 'new york city', 'french (classic)']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[764, \"buzio's in the rio\", '3700 w. flamingo rd.', 'las vegas', 'seafood']\n",
      "[542, 'carnival world', '3700 w. flamingo rd.', 'las vegas', 'buffets']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n",
      "Number of matches:  5\n",
      "Number of nonmatches:  5\n"
     ]
    }
   ],
   "source": [
    "from a2_utils import crowdsourcing\n",
    "\n",
    "# choose the most/least similar five pairs as initial training data\n",
    "init_pairs = simpairs[:5] + simpairs[-5:]\n",
    "# print(simpairs[:2])\n",
    "# print(init_pairs)\n",
    "matches = []\n",
    "nonmatches = []\n",
    "for pair in init_pairs:\n",
    "    is_match = crowdsourcing(pair)\n",
    "    if is_match == True:\n",
    "        matches.append(pair)\n",
    "    else:\n",
    "        nonmatches.append(pair)\n",
    "        \n",
    "print(\"Number of matches: \", len(matches))\n",
    "print(\"Number of nonmatches: \", len(nonmatches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the only code you need to write in this assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[741, 'palm too', '840 second ave.', 'new york city', 'steakhouses']\n",
      "[740, 'palm', '837 second ave.', 'new york city', 'steakhouses']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[217, 'ritz-carlton dining room (san francisco)', '600 stockton st.', 'san francisco', 'french (new)']\n",
      "[216, 'ritz-carlton restaurant and dining room', '600 stockton st.', 'san francisco', 'american']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[393, 'le colonial', '149 e. 57th st.', 'new york', 'asian']\n",
      "[392, 'le chantilly', '106 e. 57th st.', 'new york', 'french']\n",
      "\u001b[1;31mAnswer: No\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[84, 'lespinasse (new york city)', '2 e. 55th st.', 'new york city', 'asian']\n",
      "[83, 'lespinasse', '2 e. 55th st.', 'new york', 'american']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mAre they matching?\u001b[0m\n",
      "[150, \"bone's restaurant\", '3130 piedmont rd. ne', 'atlanta', 'steakhouses']\n",
      "[149, \"bone's\", '3130 piedmont road', 'atlanta', 'american']\n",
      "\u001b[1;31mAnswer: Yes\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from a2_utils import featurize, crowdsourcing\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import reduce \n",
    "import numpy as np\n",
    "\n",
    "labeled_pairs = matches + nonmatches\n",
    "unlabeled_pairs = [p for p in simpairs if p not in labeled_pairs]\n",
    "\n",
    "# Let Match = 1, Not Match = 0\n",
    "target_labels = [1,1,1,1,1,0,0,0,0,0]\n",
    "\n",
    "iter_num = 5\n",
    "for i in range(iter_num):\n",
    "    featurized_labeled_pairs = list(map(featurize, labeled_pairs))\n",
    "    featurized_unlabeled_pairs = list(map(featurize, unlabeled_pairs))\n",
    "\n",
    "    model = LogisticRegression()\n",
    "    model.fit(featurized_labeled_pairs,target_labels)\n",
    "    prediction_proba = model.predict_proba(featurized_unlabeled_pairs)\n",
    "    \n",
    "    # Flatten the prediction_proba\n",
    "    flatten_list_proba = reduce(lambda l1,l2: l1+l2, prediction_proba.tolist() )\n",
    "\n",
    "    uncertain_val = 0.5\n",
    "    \n",
    "    # Select only some relevant element \n",
    "    flatten_list_proba = [v for i, v in enumerate(flatten_list_proba) if i % 2 == 0]\n",
    "    \n",
    "    # Select value that is close to 0.5 (uncertain value)\n",
    "    close_to_uncertain_val  = min(flatten_list_proba, key=lambda x:abs(x-uncertain_val))\n",
    "    index_closest_to_uncertain_val = flatten_list_proba.index(close_to_uncertain_val)\n",
    "    \n",
    "    uncertain_unlabeled_pairs = unlabeled_pairs[index_closest_to_uncertain_val]\n",
    "    is_match = crowdsourcing(uncertain_unlabeled_pairs)\n",
    "    if is_match == True:\n",
    "        target_labels.append(1)\n",
    "    else:\n",
    "        target_labels.append(0)\n",
    "    labeled_pairs.append(uncertain_unlabeled_pairs)\n",
    "    unlabeled_pairs.pop(index_closest_to_uncertain_val)\n",
    "    \n",
    "#     print(len(labeled_pairs))\n",
    "#     print(len(unlabeled_pairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Algorithm Description].**   Active learning has many [query strategies](http://tiny.cc/al-wiki-qs) to decide which data point should be labeled. You need to implement uncertain sampling. The algorithm trains an initial model on `labeled_pairs`. Then, it iteratively trains a model. At each iteration, it first applies the model to `unlabeled_pairs`, and makes a prediction on each unlabeled pair along with a probability, where the probability indicates the confidence of the prediction. After that, it selects the most uncertain pair (If there is still a tie, break it randomly),  and call the `crowdsroucing()` function to label the pair. After the pair is labeled, it updates `labeled_pairs` and `unlabeled_pairs`, and then retrain the model on `labeled_pairs`.\n",
    "\n",
    "**[Input].** \n",
    "- `labeled_pairs`: 10 labeled pairs (by default)\n",
    "- `unlabeled_pairs`: 668 unlabeled pairs (by default)\n",
    "- `iter_num`: 5 (by default)\n",
    "\n",
    "**[Output].** \n",
    "- `model`: A logistic regression model built by scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training an model, you can use the following code to evalute it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9056603773584906\n",
      "Recall: 0.9056603773584906\n",
      "Fscore: 0.9056603773584906\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from a2_utils import evaluate\n",
    "\n",
    "            \n",
    "sp_features = np.array([featurize(sp) for sp in simpairs])\n",
    "label = model.predict(sp_features)\n",
    "pair_label = zip(simpairs, label)\n",
    "\n",
    "identified_matches = []\n",
    "for pair, label in pair_label:\n",
    "    if label == 1:\n",
    "        identified_matches.append(pair)\n",
    "        \n",
    "precision, recall, fscore = evaluate(identified_matches)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Fscore:\", fscore)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code in A2-2.ipynb, and submit it to the CourSys activity Assignment 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
